{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWbtiFUlChWYppSbe5kNHe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavVinayKathik/FML/blob/main/Stemming_And_Lemmatization_By_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrPtHGOCtl5o",
        "outputId": "77f46616-7d4b-4599-fd69-c8dcf85da82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --user -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdfQAY1IvPat",
        "outputId": "5155856a-be96-4724-caaf-e4c0f2f937b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_text = '''Hi All Good Morning.It's Been A Pleasant Morning for biscuits and chickens under schools'''"
      ],
      "metadata": {
        "id": "X7reJpbXvc29"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization using split()\n",
        "tokens = string_text.split()\n",
        "sentences = string_text.split('.')\n",
        "print(sentences)\n",
        "print(len(sentences))\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLEo7MZmwRod",
        "outputId": "4a0da040-0533-4c7c-c2ac-256d51d8b37f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi All Good Morning', \"It's Been A Pleasant Morning for biscuits and chickens under schools\"]\n",
            "2\n",
            "['Hi', 'All', 'Good', \"Morning.It's\", 'Been', 'A', 'Pleasant', 'Morning', 'for', 'biscuits', 'and', 'chickens', 'under', 'schools']\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization using regular expressions\n",
        "import re\n",
        "tokens = re.findall(\"[\\w']+\",string_text)\n",
        "text = '''Hi there!.Are You planning to work.'''\n",
        "token = re.findall(\"[\\w']+\",text)\n",
        "senttokens = re.compile('[!.?]').split(text)\n",
        "print(senttokens)\n",
        "print(tokens)\n",
        "print(len(tokens))\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5_f9l2KyrS7",
        "outputId": "cdb8a900-3e80-4d9f-9fa6-5dbef03b8126"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi there', '', 'Are You planning to work', '']\n",
            "['Hi', 'All', 'Good', 'Morning', \"It's\", 'Been', 'A', 'Pleasant', 'Morning', 'for', 'biscuits', 'and', 'chickens', 'under', 'schools']\n",
            "15\n",
            "['Hi', 'there', 'Are', 'You', 'planning', 'to', 'work']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization using nltk package\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "print(len(sentences))\n",
        "print(tokens)\n",
        "print(len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_iv65vrzq2k",
        "outputId": "22ab7b1c-ef54-45ed-e034-5eaad717ec58"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi there!.Are You planning to work.']\n",
            "1\n",
            "['Hi', 'there', '!', '.Are', 'You', 'planning', 'to', 'work', '.']\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming \n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"coupling\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuDK43FQ9XWB",
        "outputId": "1d849b22-7437-4715-92a3-63fa4e5af3f8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "coupl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "text = \"Hi! there, what are you waiting for?\"\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "def stemsent(sentence):\n",
        "    wordtokens = word_tokenize(sentence)\n",
        "    print(wordtokens)\n",
        "    stem_words = []\n",
        "    for i in wordtokens:\n",
        "      stem_words.append(porter.stem(i))\n",
        "      stem_words.append(\" \")\n",
        "    return \"\".join(stem_words)\n",
        "obj = stemsent(text)\n",
        "print(obj)      \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh070gYv-rsO",
        "outputId": "b1244a32-1b3e-4c10-d343-05081f75e97e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', '!', 'there', ',', 'what', 'are', 'you', 'waiting', 'for', '?']\n",
            "hi ! there , what are you wait for ? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xG_SCTRFOeAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d_IHWMNUOeq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet = WordNetLemmatizer()\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Hi! there, what are has you waiting for?\"\n",
        "def lemsent(sentence):\n",
        "  word_tokens = word_tokenize(sentence)\n",
        "  print(word_tokens)\n",
        "  lem_sent = []\n",
        "  for i in word_tokens:\n",
        "    lem_sent.append(wordnet.lemmatize(i))\n",
        "    lem_sent.append(\" \")\n",
        "  return \"\".join(lem_sent)\n",
        "\n",
        "obj = lemsent(text)\n",
        "print(obj)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVGLtIrJCxaX",
        "outputId": "7476b153-7638-4a9e-9522-c1ee210580a2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', '!', 'there', ',', 'what', 'are', 'has', 'you', 'waiting', 'for', '?']\n",
            "Hi ! there , what are ha you waiting for ? \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}